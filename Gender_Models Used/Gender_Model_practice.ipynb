{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import compute_class_weight\n",
    "#%matplotlib inline\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, Activation, Conv2D, MaxPool2D, AveragePooling2D, Flatten, Dropout, BatchNormalization\n",
    "from tensorflow.keras.optimizers import SGD, RMSprop, Adam\n",
    "\n",
    "print(\"Num GPUs Available: \", tf.config.experimental.list_physical_devices('GPU'))\n",
    "\n",
    "#pip install numba \n",
    "\n",
    "# from numba import cuda\n",
    "\n",
    "# cuda.select_device(0)\n",
    "# cuda.close()\n",
    "\n",
    "wiki_process =  pd.read_csv(\"data/dataset.csv\")\n",
    "\n",
    "wiki_process.head()\n",
    "\n",
    "start = time.time()\n",
    "try:  \n",
    "    with tf.device('/device:GPU:7'):\n",
    "        image_list = []\n",
    "        for path in wiki_process[\"img_path\"]:\n",
    "            img = cv2.imread(\"data/\" + path,cv2.IMREAD_GRAYSCALE)\n",
    "            img = cv2.resize(img,(200,200))\n",
    "            image_list.append(img)\n",
    "except RuntimeError as e:\n",
    "  print(e)\n",
    "end = time.time()\n",
    "print(\"time taken for execution :- {}\".format(end-start))\n",
    "\n",
    "# wiki_process = wiki_process.head(500)\n",
    "wiki_process[\"image\"] = image_list\n",
    "wiki_process.head()\n",
    "\n",
    "wiki_process.info()\n",
    "\n",
    "#plt.imshow(wiki_process[\"image\"][39454])\n",
    "\n",
    "#normalizing the pixel values\n",
    "try:  \n",
    "    with tf.device('/device:GPU:7'):\n",
    "        x_data = np.array(image_list)/255\n",
    "        y_data = wiki_process[\"gender\"].to_numpy()\n",
    "except RuntimeError as e:\n",
    "  print(e)\n",
    "\n",
    "x_data.shape\n",
    "\n",
    "y_data.shape\n",
    "\n",
    "# image_x will contain the original grayscale images \n",
    "x_data = x_data.reshape((x_data.shape[0],200,200,1))\n",
    "\n",
    "print(\"x_data shape: {}\".format(x_data.shape))\n",
    "print(\"y_data shape: {}\".format(y_data.shape))\n",
    "\n",
    "train_x, test_x, train_y, test_y = train_test_split(x_data, y_data, test_size=0.33, random_state=42)\n",
    "\n",
    "print(\"train_x shape: {}\".format(train_x.shape))\n",
    "print(\"train_y shape: {}\\n\".format(train_y.shape))\n",
    "\n",
    "print(\"test_x shape: {}\".format(test_x.shape))\n",
    "print(\"test_y shape: {}\".format(test_y.shape))\n",
    "\n",
    "# num_subjects = np.unique(y_data).shape[0]\n",
    "# print(\"Number of subjects: {}\".format(np.unique(y_data).shape[0]))\n",
    "\n",
    "# # Model\n",
    "\n",
    "# if tf.config.experimental.list_physical_devices('GPU'):\n",
    "#     strategy = tf.distribute.MirroredStrategy()\n",
    "# else:  # use default strategy\n",
    "#     strategy = tf.distribute.get_strategy() \n",
    "# print(strategy)\n",
    "\n",
    "# try:  \n",
    "#     with strategy.scope():\n",
    "#         # specify the input size of the images\n",
    "#         images = Input((train_x.shape[1], train_x.shape[2], 1,))\n",
    "#         # a convolution layer of 32 filters of size 9x9 to extract features (valid padding)\n",
    "#         x = Conv2D(64,kernel_size=(3,3),padding=\"valid\")(images)\n",
    "#         x = Conv2D(64,kernel_size=(3,3),padding=\"valid\")(x)\n",
    "        \n",
    "#         # a maxpooling layer to down-sample features with pool size (2, 2)\n",
    "#         x = MaxPool2D(pool_size=(2,2),strides=2)(x)\n",
    "\n",
    "#         x = Conv2D(128,kernel_size=(3,3),padding=\"valid\")(x)\n",
    "#         x = Conv2D(128,kernel_size=(3,3),padding=\"valid\")(x)\n",
    "\n",
    "#         # a maxpooling layer to down-sample features with pool size (2, 2)\n",
    "#         x = MaxPool2D(pool_size=(2,2),strides=2)(x)\n",
    "\n",
    "#         x = Conv2D(256,kernel_size=(3,3),padding=\"valid\")(x)\n",
    "#         x = Conv2D(256,kernel_size=(3,3),padding=\"valid\")(x)\n",
    "#         x = Conv2D(256,kernel_size=(3,3),padding=\"valid\")(x)\n",
    "#         x = Conv2D(256,kernel_size=(3,3),padding=\"valid\")(x)\n",
    "\n",
    "#         # a maxpooling layer to down-sample features with pool size (2, 2)\n",
    "#         x = MaxPool2D(pool_size=(2,2),strides=2)(x)\n",
    "        \n",
    "#         x = Conv2D(512,kernel_size=(3,3),padding=\"valid\")(x)\n",
    "#         x = Conv2D(512,kernel_size=(3,3),padding=\"valid\")(x)\n",
    "#         x = Conv2D(512,kernel_size=(3,3),padding=\"valid\")(x)\n",
    "#         x = Conv2D(512,kernel_size=(3,3),padding=\"valid\")(x)\n",
    "\n",
    "#         # a maxpooling layer to down-sample features with pool size (2, 2)\n",
    "#         x = MaxPool2D(pool_size=(2,2),strides=2)(x)\n",
    "        \n",
    "#         x = Conv2D(512,kernel_size=(3,3),padding=\"valid\")(x)\n",
    "#         x = Conv2D(512,kernel_size=(3,3),padding=\"valid\")(x)\n",
    "#         x = Conv2D(512,kernel_size=(3,3),padding=\"valid\")(x)\n",
    "#         x = Conv2D(512,kernel_size=(3,3),padding=\"valid\")(x)\n",
    "        \n",
    "#         # a maxpooling layer to down-sample features with pool size (2, 2)\n",
    "#         x = MaxPool2D(pool_size=(2,2),strides=2)(x)\n",
    "        \n",
    "#         # flatten extracted features to form feature vector\n",
    "#         x = Flatten()(x)\n",
    "\n",
    "# #         # a drop out layer for regularization (25% probability)\n",
    "# #         x = Dropout(rate=0.2,seed=0.25)(x)\n",
    "        \n",
    "#         first fully-connected layer to map the features to vectors of size 256\n",
    "#         x = Dense(4096,activation=\"relu\")(x)\n",
    "#         x = Dense(4096,activation=\"relu\")(x)\n",
    "#         x = Dense(128,activation=\"relu\")(x)\n",
    "        \n",
    "        \n",
    "# #         # anoter drop out layer for regularization (25% probability)\n",
    "# #         x = Dropout(rate=0.2,seed=0.25)(x)\n",
    "        \n",
    "#         # a second fully-connected layer to map the features to a logit vector with one logit per subject\n",
    "#         x = Dense(1)(x)\n",
    "#         # use softmax activation to convert the logits to class probabilities for each subject\n",
    "#         predictions = Activation(\"sigmoid\")(x)\n",
    "\n",
    "#         # create the model using the layers we defined previously\n",
    "#         sample_cnn = Model(inputs=images, outputs=predictions)\n",
    "\n",
    "#         # compile the model so that it uses Adam for optimization during training with cross-entropy loss\n",
    "#         sample_cnn.compile(optimizer=SGD(), loss=\"binary_crossentropy\", metrics=[\"acc\"])\n",
    "\n",
    "#         # print out a summary of the model achitecture\n",
    "#         print(sample_cnn.summary())\n",
    "\n",
    "# except RuntimeError as e:\n",
    "#   print(e)\n",
    "\n",
    "# start = time.time()\n",
    "# # class_weights = compute_class_weight(\"balanced\", np.unique(train_y), train_y)\n",
    "# # class_weights = dict(enumerate(class_weights))\n",
    "# try:  \n",
    "#     with tf.device('/device:GPU:4'):\n",
    "#         # train model\n",
    "#         history = sample_cnn.fit(train_x, train_y, validation_data=(test_x, test_y), epochs=1,batch_size=150, verbose=1)\n",
    "# except RuntimeError as e:\n",
    "#   print(e)\n",
    "# end = time.time()\n",
    "# print(\"Time spent for training - {}\".format(end-start))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# try:  \n",
    "#     with tf.device('/device:GPU:5'):\n",
    "#         test_pred = sample_cnn.predict(test_x)\n",
    "#         for i in test_pred:\n",
    "#             if i[0] >= 0.5:\n",
    "#                 i[0] = 1\n",
    "#             else:\n",
    "#                 i[0] = 0\n",
    "#         print(test_pred)\n",
    "# except RuntimeError as e:\n",
    "#   print(e)\n",
    "\n",
    "# test_pred[test_pred<0.5]\n",
    "\n",
    "# print(classification_report(test_y,test_pred))\n",
    "# print(confusion_matrix(test_y,test_pred))\n",
    "\n",
    "# test_y[0]\n",
    "\n",
    "# for i in test_pred:\n",
    "#     if i[0] >= 0.5:\n",
    "#         i[0] = 1\n",
    "#     else:\n",
    "#         i[0] = 0\n",
    "# test_pred\n",
    "\n",
    "# history.history.keys()\n",
    "\n",
    "# # summarize history for accuracy\n",
    "# plt.plot(history.history['acc'])\n",
    "# plt.plot(history.history['val_acc'])\n",
    "# plt.title('model accuracy')\n",
    "# plt.ylabel('accuracy')\n",
    "# plt.xlabel('epoch')\n",
    "# plt.legend(['train', 'test'], loc='upper left')\n",
    "# plt.show()\n",
    "# # summarize history for loss\n",
    "# plt.plot(history.history['loss'])\n",
    "# plt.plot(history.history['val_loss'])\n",
    "# plt.title('model loss')\n",
    "# plt.ylabel('loss')\n",
    "# plt.xlabel('epoch')\n",
    "# plt.legend(['train', 'test'], loc='upper left')\n",
    "# plt.show()\n",
    "\n",
    "# sample_cnn.save(\"models/vgg19_model\")\n",
    "\n",
    "# batch norm SGD Model\n",
    "chanDim = -1\n",
    "\n",
    "try:  \n",
    "    with tf.device('/device:GPU:7'):\n",
    "        # specify the input size of the images\n",
    "        images = Input((train_x.shape[1], train_x.shape[2], 1,))\n",
    "        x = Conv2D(32,kernel_size=(3,3),padding=\"same\")(images)\n",
    "\n",
    "        x = Activation(\"relu\")(x)\n",
    "\n",
    "        x= BatchNormalization(axis=chanDim)(x)\n",
    "        x= MaxPool2D(pool_size=(3,3))(x)\n",
    "        x= Dropout(0.25)(x)\n",
    "\n",
    "        x= Conv2D(64, (3,3), padding=\"same\")(x)\n",
    "        x= Activation(\"relu\")(x)\n",
    "        x= BatchNormalization(axis=chanDim)(x)\n",
    "        x= Conv2D(64, (3,3), padding=\"same\")(x)\n",
    "        x= Activation(\"relu\")(x)\n",
    "        x= BatchNormalization(axis=chanDim)(x)\n",
    "        x= MaxPool2D(pool_size=(2,2))(x)\n",
    "        x= Dropout(0.25)(x)\n",
    "\n",
    "        x= Conv2D(128, (3,3), padding=\"same\")(x)\n",
    "        x= Activation(\"relu\")(x)\n",
    "        x= BatchNormalization(axis=chanDim)(x)\n",
    "\n",
    "        x= Conv2D(128, (3,3), padding=\"same\")(x)\n",
    "        x= Activation(\"relu\")(x)\n",
    "        x= BatchNormalization(axis=chanDim)(x)\n",
    "        x= MaxPool2D(pool_size=(2,2))(x)\n",
    "        x= Dropout(0.25)(x)\n",
    "\n",
    "        x= Flatten()(x)\n",
    "        x= Dense(1024)(x)\n",
    "        x= Activation(\"relu\")(x)\n",
    "        x= BatchNormalization(axis=chanDim)(x)\n",
    "        x= Dropout(0.5)(x)\n",
    "\n",
    "        x= Dense(1)(x)\n",
    "        \n",
    "        predictions = Activation(\"sigmoid\")(x)\n",
    "\n",
    "        # create the model using the layers we defined previously\n",
    "        sample_cnn = Model(inputs=images, outputs=predictions)\n",
    "\n",
    "        # compile the model so that it uses Adam for optimization during training with cross-entropy loss\n",
    "        sample_cnn.compile(optimizer=SGD(), loss=\"binary_crossentropy\", metrics=[\"acc\"])\n",
    "\n",
    "        # print out a summary of the model achitecture\n",
    "        print(sample_cnn.summary())\n",
    "\n",
    "except RuntimeError as e:\n",
    "  print(e)\n",
    "\n",
    "start = time.time()\n",
    "# class_weights = compute_class_weight(\"balanced\", np.unique(train_y), train_y)\n",
    "# class_weights = dict(enumerate(class_weights))\n",
    "try:  \n",
    "    with tf.device('/device:GPU:7'):\n",
    "        # train model\n",
    "        history = sample_cnn.fit(train_x, train_y, validation_data=(test_x, test_y), epochs=100,batch_size=100, verbose=1)\n",
    "except RuntimeError as e:\n",
    "  print(e)\n",
    "end = time.time()\n",
    "print(\"Time spent for training - {}\".format(end-start))\n",
    "\n",
    "try:  \n",
    "    with tf.device('/device:GPU:7'):\n",
    "        test_pred = sample_cnn.predict(test_x)\n",
    "        for i in test_pred:\n",
    "            if i[0] >= 0.5:\n",
    "                i[0] = 1\n",
    "            else:\n",
    "                i[0] = 0\n",
    "        print(test_pred)\n",
    "except RuntimeError as e:\n",
    "  print(e)\n",
    "\n",
    "print(classification_report(test_y,test_pred))\n",
    "print(confusion_matrix(test_y,test_pred))\n",
    "\n",
    "# summarize history for accuracy\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "sample_cnn.save(\"models/batch_norm_sgd\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
