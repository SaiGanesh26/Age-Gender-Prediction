{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/SaiGanesh26/Gender-Classification-Age-Prediction/blob/master/AgeGender.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1m8Da4OSQqig"
   },
   "source": [
    "# Age Prediction & Gender Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PBe-4781Q0UB"
   },
   "source": [
    "###Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fNTkr1Bf3Lw0"
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model\n",
    "# from google.colab import  files\n",
    "# from google.colab.patches import cv2_imshow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "42nhVGJSUP0P"
   },
   "source": [
    "####Using OpenCV haar cascade classifiers for face detection "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LmNcALLkROT9"
   },
   "outputs": [],
   "source": [
    "face_cascade = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "d0xS_sn8Uqtf"
   },
   "source": [
    "####Detecting and drawing shapes across the face\n",
    "* Face is detected across each frame if video input is given"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yyZo8ZUgRShx"
   },
   "outputs": [],
   "source": [
    "def detect_face_image(img):\n",
    "    label_dict={0:'Female',1:'Male'}\n",
    "    color_dict={0:(0,255,0),1:(0,0,255)}\n",
    "    \n",
    "    face_coord = face_cascade.detectMultiScale(img,scaleFactor = 1.2,minNeighbors = 5)  # This gives us the coordinates of rectangle drawn across the face \n",
    "    for x,y,w,h in face_coord:\n",
    "        image = img[y:y+h,x:x+h]\n",
    "        image_gry = cv2.cvtColor(image,cv2.COLOR_BGR2GRAY)\n",
    "        resize_img = cv2.resize(image_gry,(200,200))\n",
    "        norm_img = resize_img/255.0\n",
    "        reshaped=np.reshape(norm_img,(1,200,200,1))\n",
    "        #reshaped = np.vstack([reshaped])\n",
    "\n",
    "        try:  \n",
    "            with tf.device('/device:GPU:5'):\n",
    "                gender_res = gen_model.predict(reshaped)\n",
    "        except RuntimeError as e:\n",
    "            print(e)\n",
    "        if(gender_res[0][0]<0.5):\n",
    "            label=0\n",
    "        else:\n",
    "            label=1\n",
    "     \n",
    "        cv2.rectangle(img,(x,y),(x+w,y+h),color_dict[label],thickness = 5)\n",
    "        cv2.rectangle(img,(x,y-40),(x+w,y),color_dict[label],-1)\n",
    "        cv2.putText(img, label_dict[label], (x, y-40),cv2.FONT_HERSHEY_SIMPLEX,5,(255,255,255),5)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# image = cv2.imread(\"IMG_2137.JPG\")\n",
    "# out_image = detect_face_image(image)\n",
    "# plt.imshow(out_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Tk8gMT1RX8Fv"
   },
   "source": [
    "####Video Input\n",
    "* If existing video file is to be used then file path is given\n",
    "* If live web cam is used just enter 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6aVohCYPX6Nw"
   },
   "outputs": [],
   "source": [
    "video = cv2.VideoCapture('test3.mp4')  # Sincle colab has no option for live recording, existing file is used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_face_video(img):\n",
    "    label_dict={0:'Female',1:'Male'}\n",
    "    color_dict={0:(0,255,0),1:(0,0,255)}\n",
    "    \n",
    "    face_coord = face_cascade.detectMultiScale(img,scaleFactor = 1.2,minNeighbors = 5)  # This gives us the coordinates of rectangle drawn across the face \n",
    "    for x,y,w,h in face_coord:\n",
    "        image = img[y:y+h,x:x+h]\n",
    "        image_gry = cv2.cvtColor(image,cv2.COLOR_BGR2GRAY)\n",
    "        resize_img = cv2.resize(image_gry,(200,200))\n",
    "        norm_img = resize_img/255.0\n",
    "        reshaped=np.reshape(norm_img,(1,200,200,1))\n",
    "        #reshaped = np.vstack([reshaped])\n",
    "\n",
    "        try:  \n",
    "            with tf.device('/device:GPU:5'):\n",
    "                gender_res = gen_model.predict(reshaped)\n",
    "        except RuntimeError as e:\n",
    "            print(e)\n",
    "        if(gender_res[0][0]<0.5):\n",
    "            label=0\n",
    "        else:\n",
    "            label=1\n",
    "     \n",
    "        cv2.rectangle(img,(x,y),(x+w,y+h),color_dict[label],thickness = 5)\n",
    "        cv2.rectangle(img,(x,y-40),(x+w,y),color_dict[label],-1)\n",
    "        cv2.putText(img, label_dict[label], (x, y-10),cv2.FONT_HERSHEY_SIMPLEX,0.8,(255,255,255),2)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ohs7zYxbK621"
   },
   "outputs": [],
   "source": [
    "width = int(video.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "height = int(video.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "out = cv2.VideoWriter('test3_out.mp4',cv2.VideoWriter_fourcc(*'XVID'),25,(width,height))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dFzeit-Hb6U0"
   },
   "source": [
    "* colab gives the output of video files as a series of each frame present in video file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:  \n",
    "    with tf.device('/device:GPU:5'):\n",
    "        gen_model = load_model(\"models/cnn1_relu_sgd_84acc_80tr_20te_less_false_neg\")\n",
    "except RuntimeError as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xM-vCzDiZoT_"
   },
   "outputs": [],
   "source": [
    "if video.isOpened() == False:\n",
    "  print('Error! File Not found')\n",
    "while video.isOpened():\n",
    "    ret,frame = video.read()\n",
    "    frame = detect_face_video(frame)  #detect the face for each frame displayed on video\n",
    "    if ret == True:\n",
    "        out.write(frame)\n",
    "#     if cv2.waitKey(27) & 0xFF == ord('q'): #if live webcam is given input then video can be quit using letter 'q'\n",
    "#         break\n",
    "    else:\n",
    "        break\n",
    "video.release()\n",
    "out.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vwVn-IerbZH3"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNdoMOU+KjofJthu5WWRmd6",
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "AgeGender.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
